{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7c9cdb0-64aa-4d17-aa69-a043d8e54074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing at 2025-04-11 22:17:33.123894\n",
      "Found 365 files to process for the selected year\n",
      "Using parallel processing with Dask...\n",
      "Created dataframe with 113643747 total points\n",
      "Creating 2.5째 x 2.5째 spatial bins...\n",
      "Calculating daily statistics for each bin...\n",
      "Creating bin geometries...\n",
      "Created binned statistics with 1244955 rows\n",
      "Adding ocean names to bins...\n",
      "Reading ocean shapefile from ne_10m_geography_marine_polys.shp...\n",
      "Available columns in shapefile: ['featurecla', 'name', 'namealt', 'changed', 'note', 'name_fr', 'min_label', 'max_label', 'scalerank', 'label', 'wikidataid', 'name_ar', 'name_bn', 'name_de', 'name_en', 'name_es', 'name_el', 'name_hi', 'name_hu', 'name_id', 'name_it', 'name_ja', 'name_ko', 'name_nl', 'name_pl', 'name_pt', 'name_ru', 'name_sv', 'name_tr', 'name_vi', 'name_zh', 'ne_id', 'name_fa', 'name_he', 'name_uk', 'name_ur', 'name_zht', 'geometry']\n",
      "Using 'name' as the name column\n",
      "Using 'featurecla' as the feature class column\n",
      "Performing spatial join...\n",
      "Found 4854 matches for 3418 bins (some bins match multiple oceans)\n",
      "For each bin, keeping the ocean with the largest overlapping area...\n",
      "Added ocean names from 'name' column\n",
      "Added feature class from 'featurecla' column\n",
      "Added ocean information to 1244955/1244955 rows\n",
      "Daily microplastic concentration data saved to ./output\\microplastics_daily_2.5deg_2019.csv\n",
      "Processing completed in 0:15:36.615089\n",
      "Files ready for visualization\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from shapely.geometry import Polygon\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm  # For progress bars\n",
    "import dask\n",
    "\n",
    "def extract_date_from_filename(filename):\n",
    "    \"\"\"Extract date from the NASA filename convention\"\"\"\n",
    "    match = re.search(r\"s(\\d{8})\", os.path.basename(filename))\n",
    "    if match:\n",
    "        date_str = match.group(1)\n",
    "        return pd.to_datetime(date_str, format='%Y%m%d')\n",
    "    return None\n",
    "\n",
    "def process_nc4_files(data_dir, year=None, parallel=True):\n",
    "    \"\"\"\n",
    "    Process NC4 files for all days in a specified year or all files if year is None\n",
    "    Returns a dataframe with raw points\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing NC4/NC files\n",
    "        year: Optional year to filter (e.g., '2019')\n",
    "        parallel: Whether to use parallel processing (requires dask)\n",
    "    \"\"\"\n",
    "    # Find all NC4 files in the directory\n",
    "    nc4_files = glob.glob(os.path.join(data_dir, \"*.nc4\"))\n",
    "    if not nc4_files:\n",
    "        # Try with .nc extension if .nc4 not found\n",
    "        nc4_files = glob.glob(os.path.join(data_dir, \"*.nc\"))\n",
    "    \n",
    "    if not nc4_files:\n",
    "        raise ValueError(f\"No NC4 or NC files found in {data_dir}\")\n",
    "    \n",
    "    # Filter files by year if specified\n",
    "    if year:\n",
    "        filtered_files = []\n",
    "        for file_path in nc4_files:\n",
    "            file_date = extract_date_from_filename(file_path)\n",
    "            if file_date and file_date.strftime('%Y') == year:\n",
    "                filtered_files.append(file_path)\n",
    "        nc4_files = filtered_files\n",
    "    \n",
    "    print(f\"Found {len(nc4_files)} files to process for the selected year\")\n",
    "    \n",
    "    if parallel and len(nc4_files) > 10:\n",
    "        # Parallel processing using dask if available and we have many files\n",
    "        try:\n",
    "            import dask\n",
    "            import dask.dataframe as dd\n",
    "            from dask import delayed\n",
    "            \n",
    "            print(\"Using parallel processing with Dask...\")\n",
    "            # Define function to process a single file\n",
    "            @delayed\n",
    "            def process_single_file(file_path):\n",
    "                try:\n",
    "                    file_date = extract_date_from_filename(file_path)\n",
    "                    if file_date is None:\n",
    "                        return None\n",
    "                    \n",
    "                    # Load NC4 file\n",
    "                    ds = xr.open_dataset(file_path)\n",
    "                    \n",
    "                    # Convert to DataFrame - only extract needed columns\n",
    "                    df = ds[[\"lat\", \"lon\", \"mp_concentration\"]].to_dataframe().reset_index()\n",
    "                    \n",
    "                    # Drop any rows with missing microplastic data\n",
    "                    df = df.dropna(subset=[\"mp_concentration\"])\n",
    "                    \n",
    "                    # Convert longitude from 0-360 to -180 to 180 range if needed\n",
    "                    df['lon'] = (df['lon'] + 180) % 360 - 180\n",
    "                    \n",
    "                    # Add date column - this time with day precision\n",
    "                    df['date'] = file_date\n",
    "                    \n",
    "                    return df\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {str(e)}\")\n",
    "                    return None\n",
    "            \n",
    "            # Process all files in parallel\n",
    "            dfs = [process_single_file(f) for f in nc4_files]\n",
    "            # Compute and combine results\n",
    "            results = dask.compute(*dfs)\n",
    "            # Filter out None results\n",
    "            valid_results = [df for df in results if df is not None]\n",
    "            \n",
    "            if not valid_results:\n",
    "                raise ValueError(\"No valid data frames were produced from the NC4 files\")\n",
    "            \n",
    "            # Combine all dataframes\n",
    "            points_df = pd.concat(valid_results, ignore_index=True)\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"Dask not available. Falling back to sequential processing.\")\n",
    "            parallel = False\n",
    "    \n",
    "    if not parallel:\n",
    "        # Process files sequentially with progress bar\n",
    "        all_points = []\n",
    "        \n",
    "        for file_path in tqdm(nc4_files, desc=\"Processing files\"):\n",
    "            try:\n",
    "                # Extract date from filename\n",
    "                file_date = extract_date_from_filename(file_path)\n",
    "                \n",
    "                if file_date is None:\n",
    "                    print(f\"Could not extract date from {file_path}\")\n",
    "                    continue\n",
    "                \n",
    "                # Load NC4 file\n",
    "                ds = xr.open_dataset(file_path)\n",
    "                \n",
    "                # Convert to DataFrame - only extract needed columns\n",
    "                df = ds[[\"lat\", \"lon\", \"mp_concentration\"]].to_dataframe().reset_index()\n",
    "                \n",
    "                # Drop any rows with missing microplastic data\n",
    "                df = df.dropna(subset=[\"mp_concentration\"])\n",
    "                \n",
    "                # Convert longitude from 0-360 to -180 to 180 range if needed\n",
    "                df['lon'] = (df['lon'] + 180) % 360 - 180\n",
    "                \n",
    "                # Add date column - this time with day precision\n",
    "                df['date'] = file_date\n",
    "                \n",
    "                # Add to the collection of all points\n",
    "                all_points.append(df)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_points:\n",
    "            raise ValueError(\"No valid data frames were produced from the NC4 files\")\n",
    "        \n",
    "        # Combine all dataframes with raw points\n",
    "        print(\"Combining all data points...\")\n",
    "        points_df = pd.concat(all_points, ignore_index=True)\n",
    "    \n",
    "    print(f\"Created dataframe with {len(points_df)} total points\")\n",
    "    return points_df\n",
    "\n",
    "def create_spatial_bins(points_df, bin_size=2.5, output_raw_path=None):\n",
    "    \"\"\"\n",
    "    Create spatial bins of the specified size (in degrees) and aggregate statistics\n",
    "    with daily precision\n",
    "    \"\"\"\n",
    "    print(f\"Creating {bin_size}째 x {bin_size}째 spatial bins...\")\n",
    "    \n",
    "    # Create bin columns\n",
    "    points_df['lat_bin'] = (np.floor(points_df['lat'] / bin_size) * bin_size) + (bin_size / 2)\n",
    "    points_df['lon_bin'] = (np.floor(points_df['lon'] / bin_size) * bin_size) + (bin_size / 2)\n",
    "    \n",
    "    # Create bin_id for easier reference\n",
    "    points_df['bin_id'] = points_df['lat_bin'].astype(str) + '_' + points_df['lon_bin'].astype(str)\n",
    "    \n",
    "    # Save raw points with bin info if requested\n",
    "    if output_raw_path:\n",
    "        points_df.to_csv(output_raw_path, index=False)\n",
    "        print(f\"Raw points with bin info saved to {output_raw_path}\")\n",
    "    \n",
    "    # Group by lat_bin, lon_bin, and date (daily) to calculate statistics\n",
    "    print(\"Calculating daily statistics for each bin...\")\n",
    "    binned_stats = points_df.groupby(['lat_bin', 'lon_bin', 'bin_id', 'date']).agg({\n",
    "        'mp_concentration': ['mean', 'min', 'max', 'std', 'count'],\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten the column names\n",
    "    binned_stats.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col for col in binned_stats.columns]\n",
    "    \n",
    "    # Create geometries for each bin\n",
    "    print(\"Creating bin geometries...\")\n",
    "    geometries = []\n",
    "    for lon, lat in zip(binned_stats['lon_bin'], binned_stats['lat_bin']):\n",
    "        half_bin = bin_size / 2\n",
    "        geometries.append(Polygon([\n",
    "            (lon - half_bin, lat - half_bin),\n",
    "            (lon + half_bin, lat - half_bin),\n",
    "            (lon + half_bin, lat + half_bin),\n",
    "            (lon - half_bin, lat + half_bin),\n",
    "            (lon - half_bin, lat - half_bin)\n",
    "        ]))\n",
    "    \n",
    "    # Add WKT geometry string for easy CSV use with Plotly\n",
    "    binned_stats['geometry'] = [g.wkt for g in geometries]\n",
    "    \n",
    "    print(f\"Created binned statistics with {len(binned_stats)} rows\")\n",
    "    return binned_stats\n",
    "\n",
    "def add_ocean_names(binned_stats, ocean_shapefile_path, bin_size=5.0):\n",
    "    \"\"\"\n",
    "    Add ocean names to each bin using a spatial join with an ocean shapefile\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Adding ocean names to bins...\")\n",
    "        \n",
    "        # First, extract unique bin definitions to reduce processing\n",
    "        unique_bins = binned_stats[['lat_bin', 'lon_bin', 'bin_id']].drop_duplicates()\n",
    "        \n",
    "        # Create geometries for these unique bins\n",
    "        geometries = []\n",
    "        for _, row in unique_bins.iterrows():\n",
    "            lon, lat = row['lon_bin'], row['lat_bin']\n",
    "            half_bin = bin_size / 2\n",
    "            geometries.append(Polygon([\n",
    "                (lon - half_bin, lat - half_bin),\n",
    "                (lon + half_bin, lat - half_bin),\n",
    "                (lon + half_bin, lat + half_bin),\n",
    "                (lon - half_bin, lat + half_bin),\n",
    "                (lon - half_bin, lat - half_bin)\n",
    "            ]))\n",
    "        \n",
    "        # Create a GeoDataFrame with unique bins\n",
    "        gdf_bins = gpd.GeoDataFrame(\n",
    "            unique_bins,\n",
    "            geometry=geometries,\n",
    "            crs=\"EPSG:4326\"\n",
    "        )\n",
    "        \n",
    "        # Read the ocean shapefile - select only needed columns\n",
    "        print(f\"Reading ocean shapefile from {ocean_shapefile_path}...\")\n",
    "        oceans = gpd.read_file(ocean_shapefile_path)\n",
    "        \n",
    "        # Check available columns and select appropriate ones\n",
    "        available_columns = oceans.columns.tolist()\n",
    "        print(f\"Available columns in shapefile: {available_columns}\")\n",
    "        \n",
    "        # Try to find name and feature class columns\n",
    "        name_col = next((col for col in available_columns if 'name' in col.lower()), None)\n",
    "        feature_col = next((col for col in available_columns if 'feature' in col.lower() or 'class' in col.lower()), None)\n",
    "        \n",
    "        if name_col:\n",
    "            print(f\"Using '{name_col}' as the name column\")\n",
    "        else:\n",
    "            print(\"No name column found in shapefile\")\n",
    "            name_col = available_columns[0]  # Use first column as fallback\n",
    "            print(f\"Using '{name_col}' as fallback name column\")\n",
    "        \n",
    "        if feature_col:\n",
    "            print(f\"Using '{feature_col}' as the feature class column\")\n",
    "        else:\n",
    "            print(\"No feature class column found in shapefile\")\n",
    "            feature_col = None\n",
    "        \n",
    "        # Select relevant columns for the join\n",
    "        columns_to_keep = [\"geometry\"]\n",
    "        if name_col: columns_to_keep.append(name_col)\n",
    "        if feature_col: columns_to_keep.append(feature_col)\n",
    "        \n",
    "        oceans = oceans[columns_to_keep]\n",
    "        \n",
    "        # Perform spatial join directly with the original polygons\n",
    "        # This avoids centroid calculation warnings and is more accurate for large bins\n",
    "        print(\"Performing spatial join...\")\n",
    "        joined = gpd.sjoin(gdf_bins, oceans, how=\"left\", predicate=\"intersects\")\n",
    "        \n",
    "        # If there are multiple matches (bin intersects multiple oceans), keep the one with largest intersection\n",
    "        if len(joined) > len(gdf_bins):\n",
    "            print(f\"Found {len(joined)} matches for {len(gdf_bins)} bins (some bins match multiple oceans)\")\n",
    "            print(\"For each bin, keeping the ocean with the largest overlapping area...\")\n",
    "            \n",
    "            # For each bin with multiple ocean matches, keep the one with largest intersection area\n",
    "            # First, group by bin_id\n",
    "            groups = joined.groupby('bin_id')\n",
    "            \n",
    "            # Initialize a list to hold the rows we want to keep\n",
    "            rows_to_keep = []\n",
    "            \n",
    "            for bin_id, group in groups:\n",
    "                if len(group) == 1:\n",
    "                    # If only one match, keep it\n",
    "                    rows_to_keep.append(group.iloc[0])\n",
    "                else:\n",
    "                    # If multiple matches, calculate intersection area with each ocean\n",
    "                    bin_geom = gdf_bins.loc[gdf_bins['bin_id'] == bin_id, 'geometry'].iloc[0]\n",
    "                    \n",
    "                    # Find ocean with largest intersection\n",
    "                    max_area = 0\n",
    "                    best_row = None\n",
    "                    \n",
    "                    for _, row in group.iterrows():\n",
    "                        # Get the ocean geometry corresponding to this row\n",
    "                        ocean_geom = oceans.loc[oceans.index == row.index_right, 'geometry'].iloc[0]\n",
    "                        \n",
    "                        # Calculate intersection area\n",
    "                        intersection = bin_geom.intersection(ocean_geom)\n",
    "                        area = intersection.area\n",
    "                        \n",
    "                        if area > max_area:\n",
    "                            max_area = area\n",
    "                            best_row = row\n",
    "                    \n",
    "                    rows_to_keep.append(best_row)\n",
    "            \n",
    "            # Create a new DataFrame from the kept rows\n",
    "            joined = gpd.GeoDataFrame(rows_to_keep)\n",
    "        \n",
    "        # Create dictionaries to map bin_id to ocean name and feature class\n",
    "        ocean_name_map = {}\n",
    "        ocean_feature_map = {}\n",
    "        \n",
    "        # Create the mappings\n",
    "        for _, row in joined.iterrows():\n",
    "            if name_col and pd.notna(row[name_col]):\n",
    "                ocean_name_map[row['bin_id']] = row[name_col]\n",
    "            \n",
    "            if feature_col and pd.notna(row[feature_col]):\n",
    "                ocean_feature_map[row['bin_id']] = row[feature_col]\n",
    "        \n",
    "        # Add ocean names to the original dataframe\n",
    "        if name_col:\n",
    "            binned_stats['ocean_name'] = binned_stats['bin_id'].map(ocean_name_map)\n",
    "            print(f\"Added ocean names from '{name_col}' column\")\n",
    "        \n",
    "        # Add feature class if available\n",
    "        if feature_col:\n",
    "            binned_stats['ocean_feature'] = binned_stats['bin_id'].map(ocean_feature_map)\n",
    "            print(f\"Added feature class from '{feature_col}' column\")\n",
    "        \n",
    "        # Count how many rows got ocean names\n",
    "        ocean_count = binned_stats['ocean_name'].notna().sum() if name_col else 0\n",
    "        print(f\"Added ocean information to {ocean_count}/{len(binned_stats)} rows\")\n",
    "        \n",
    "        return binned_stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error adding ocean names: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        print(\"Continuing without ocean information.\")\n",
    "        return binned_stats\n",
    "def main():\n",
    "    # Define parameters\n",
    "    data_dir = \".\"  # Replace with your actual data directory\n",
    "    year = \"2019\"  # Process all of 2019\n",
    "    bin_size = 2.5  # Choose bin size\n",
    "    output_dir = \"./output\"  # Output directory\n",
    "    ocean_shapefile = \"ne_10m_geography_marine_polys.shp\"  # Optional: path to ocean shapefile for adding ocean names\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Process files for the selected year\n",
    "    start_time = datetime.now()\n",
    "    print(f\"Starting processing at {start_time}\")\n",
    "    \n",
    "    points_df = process_nc4_files(data_dir, year)\n",
    "    \n",
    "    # Optional: save sample of raw points for verification\n",
    "    points_sample = points_df.sample(min(10000, len(points_df)))\n",
    "    points_sample.to_csv(os.path.join(output_dir, 'raw_points_sample.csv'), index=False)\n",
    "    \n",
    "    # Step 2: Create spatial bins and calculate daily statistics\n",
    "    binned_stats = create_spatial_bins(points_df, bin_size)\n",
    "    \n",
    "    # Step 3: Add ocean names if shapefile is provided\n",
    "    if ocean_shapefile:\n",
    "        binned_stats = add_ocean_names(binned_stats, ocean_shapefile)\n",
    "    \n",
    "    # Step 4: Save final output CSV for Plotly\n",
    "    output_csv = os.path.join(output_dir, f'microplastics_daily_{bin_size}deg_{year}.csv')\n",
    "    binned_stats.to_csv(output_csv, index=False)\n",
    "    print(f\"Daily microplastic concentration data saved to {output_csv}\")\n",
    "    \n",
    "    # Print execution time\n",
    "    end_time = datetime.now()\n",
    "    print(f\"Processing completed in {end_time - start_time}\")\n",
    "    print(\"Files ready for visualization\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a16ad-82fc-4be2-907b-dd720fc7462d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv310)",
   "language": "python",
   "name": "myenv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
